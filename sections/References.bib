% % % My publications -----------------------------------------------

@article{nikolaiev2025synth,
    author = {Ніколаєв, Андрій Д. and Анісімов, Анатолій В.},
    title = {Нейромережеві методи відбору та генерації синтетичних варіацій комбінаторних задач},
    journal={Cybernetics and Systems Analysis},
    volume={41},
    number={1},
    pages={22–32},
    url={https://doi.org/10.34229/KCA2522-9664.25.3.3},
    DOI={10.34229/KCA2522-9664.25.3.3},
    year={2025}
}

@article{nikolaiev2024neuralform,
    title={Нейромережеві методи формалізації математичних текстів},
    author={Ніколаєв, Андрій},
    volume={345},
    number={6(2)},
    journal={Herald of Khmelnytskyi National University. Technical sciences},
    year={2024},
    month={Nov.},
    pages={50–55},
    url={https://doi.org/10.31891/2307-5732-2024-345-6-6},
    DOI={10.31891/2307-5732-2024-345-6-6}
}

@misc{nikolaiev2024can,
    title = {Can language models rival mathematics students? Evaluating mathematical reasoning through textual manipulation and human experiments},
    author = {Nikolaiev, Andrii and Stathopoulos, Yiannos and Teufel, Simone},
    year = {2024},
    eprint = {2412.11908},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
    url = {https://arxiv.org/abs/2412.11908}
}

@article{nikolaiev2024comparison,
    title = {Comparison of Problem-solving Performance Across Mathematical Domains With Large Language Models},
    author = {Nikolaiev, Andrii D. and Derevianchenko, Oleksandr V.},
    year = {2024},
    journal={Shtuchnyy intelekt},
    url={https://doi.org/10.15407/jai2024.04.096},
    pages={96-104}
}

@conference{Nikolaiev202294,
    title = {Mathematical word problem solution evaluation via data preprocessing approach},
    author = {Nikolaiev, Andrii D. and Anisimov, Anatoliy V.},
    booktitle = {8th International Scientific Conference "Information Technology and Implementation", IT and I 2021},
    year = {2022},
    journal = {CEUR Workshop Proceedings},
    volume = {3132},
    pages = {94 – 103},
    url = {https://ceur-ws.org/Vol-3132/Paper_9.pdf}
}

@article{vlasenko2019methodology,
    author = {Власенко, О.В. and Картавих, В.Ю. and Ніколаєв, А.Д. and Горбенко, В.І.},
    title = {Методика визначення опорної матриці моніторингу домена управління інформаційної мережі спеціального призначення},
    journal = {Системи і технології зв’язку, інформатизації та кібербезпеки. Збірник наукових праць ВІТІ, випуск 4},
    year = {2019},
    pages = {46-57}
}

% % % 1 OVERVIEW -----------------------------------------------

@article{glushkov1970some,
    author={Glushkov, V. M.},
    title={Some problems in the theories of automata and artificial intelligence},
    journal={Cybernetics},
    year={1970},
    month={Mar},
    day={01},
    volume={6},
    number={2},
    pages={17-27},
    issn={1573-8337},
    doi={10.1007/BF01070496},
    url={https://doi.org/10.1007/BF01070496}
}

@article{Glushkov1972,
    author={Glushkov, V. M.
    and Kapitonova, Yu. V.},
    title={Automatic search for proofs of mathematical theorems and intelligent computers},
    journal={Cybernetics},
    year={1972},
    month={Sep},
    day={01},
    volume={8},
    number={5},
    pages={709-713},
    issn={1573-8337},
    doi={10.1007/BF01068443},
    url={https://doi.org/10.1007/BF01068443}
}

@article{Glushkov1970,
    author={Glushkov, V. M.
    and Kapitonova, Yu. V.
    and Letichevskii, A. A.},
    title={Software for an automatic system of design of computers and systems (PROEKT)},
    journal={Cybernetics},
    year={1970},
    month={Jul},
    day={01},
    volume={6},
    number={4},
    pages={363-368},
    issn={1573-8337},
    doi={10.1007/BF01073232},
    url={https://doi.org/10.1007/BF01073232}
}

@article{glushovsad1980,
    author = {Victor M. Glushkov},
    title = {The SAD automated proving system: A brief informal presentation},
    journal = {Automated Processing of Mathematical Texts},
    address={V. M. Glushkov Institute of Cybernetics AS USSR, Kyiv},
    pages = {3--30},
    year = {1980}
}

@article{paulson1986natural,
  title = {Natural deduction as higher-order resolution},
  author = {Paulson, L. C.},
  journal = {The Journal of Logic Programming},
  volume = {3},
  number = {3},
  pages = {237--258},
  year = {1986},
  doi = {10.1016/0743-1066(86)90015-4},
  url = {https://doi.org/10.1016/0743-1066(86)90015-4},
}

@inproceedings{demoura2015lean,
  title = {The Lean theorem prover (system description)},
  author = {De Moura, Leonardo and Kong, Soonho and Avigad, Jeremy and Van Doorn, Floris and Von Raumer, Jakob},
  booktitle = {International Conference on Automated Deduction},
  pages = {378--388},
  year = {2015},
  publisher = {Springer},
  doi = {10.1007/978-3-319-21401-6_26},
  url = {https://doi.org/10.1007/978-3-319-21401-6_26},
}

@inproceedings{10.1007/978-3-540-73595-3_29,
  title = {System for automated deduction (SAD): a tool for proof verification},
  author = {Verchinine, Konstantin and Lyaletski, Alexander and Paskevich, Andrei},
  booktitle = {International Conference on Automated Deduction},
  pages = {398--403},
  year = {2007},
  publisher = {Springer},
  doi = {10.1007/978-3-540-73595-3_29},
  url = {https://doi.org/10.1007/978-3-540-73595-3_29},
}

@inproceedings{10.1007/978-3-540-85110-3_47,
  title = {On correctness of mathematical texts from a logical and practical point of view},
  author = {Verchinine, Konstantin and Lyaletski, Alexander and Paskevich, Andrey and Anisimov, Anatoly},
  booktitle = {International Conference on Intelligent Computer Mathematics},
  pages = {583--598},
  year = {2008},
  publisher = {Springer},
  doi = {10.1007/978-3-540-85110-3_47},
  url = {https://doi.org/10.1007/978-3-540-85110-3_47},
}

@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI team},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@misc{brown2020languagemodelsfewshotlearners,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}

@misc{polu2020generativelanguagemodelingautomated,
      title={Generative Language Modeling for Automated Theorem Proving}, 
      author={Stanislas Polu and Ilya Sutskever},
      year={2020},
      eprint={2009.03393},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2009.03393}, 
}

@misc{han2022proofartifactcotrainingtheorem,
      title={Proof Artifact Co-training for Theorem Proving with Language Models}, 
      author={Jesse Michael Han and Jason Rute and Yuhuai Wu and Edward W. Ayers and Stanislas Polu},
      year={2022},
      eprint={2102.06203},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2102.06203}, 
}

@inproceedings{wang-etal-2023-dt,
    title = "{DT}-Solver: Automated Theorem Proving with Dynamic-Tree Sampling Guided by Proof-level Value Function",
    author = "Wang, Haiming  and
      Yuan, Ye  and
      Liu, Zhengying  and
      Shen, Jianhao  and
      Yin, Yichun  and
      Xiong, Jing  and
      Xie, Enze  and
      Shi, Han  and
      Li, Yujun  and
      Li, Lin  and
      Yin, Jian  and
      Li, Zhenguo  and
      Liang, Xiaodan",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.706/",
    doi = "10.18653/v1/2023.acl-long.706",
    pages = "12632--12646",
    abstract = "Recent advances in neural theorem-proving resort to large language models and tree searches. When proving a theorem, a language model advises single-step actions based on the current proving state and the tree search finds a sequence of correct steps using actions given by the language model. However, prior works often conduct constant computation efforts for each proving state while ignoring that the hard states often need more exploration than easy states. Moreover, they evaluate and guide the proof search solely depending on the current proof state instead of considering the whole proof trajectory as human reasoning does. Here, to accommodate general theorems, we propose a novel Dynamic-Tree Driven Theorem Solver (DT-Solver) by guiding the search procedure with state confidence and proof-level values. Specifically, DT-Solver introduces a dynamic-tree Monte-Carlo search algorithm, which dynamically allocates computing budgets for different state confidences, guided by a new proof-level value function to discover proof states that require substantial exploration. Experiments on two popular theorem-proving datasets, PISA and Mathlib, show significant performance gains by our DT-Solver over the state-of-the-art approaches, with a 6.65{\%} improvement on average in terms of success rate. And especially under low computing resource settings (11.03{\%} improvement on average)."
}

@misc{wang2018translatingmathwordproblem,
      title={Translating a Math Word Problem to an Expression Tree}, 
      author={Lei Wang and Yan Wang and Deng Cai and Dongxiang Zhang and Xiaojiang Liu},
      year={2018},
      eprint={1811.05632},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1811.05632}, 
}

@article{Wang_Zhang_Gao_Song_Guo_Shen_2018,
    title={MathDQN: Solving Arithmetic Word Problems via Deep Reinforcement Learning},
    volume={32},
    url={https://ojs.aaai.org/index.php/AAAI/article/view/11981},
    DOI={10.1609/aaai.v32i1.11981},
    abstractNote={ &lt;p&gt; &lt;pre&gt;Designing an automatic solver for math word problems has been considered as a crucial step towards general AI, with the ability of natural language understanding and logical inference. The state-of-the-art performance was achieved by enumerating all the possible expressions from the quantities in the text and customizing a scoring function to identify the one with the maximum probability. However, it incurs exponential search space with the number of quantities and beam search has to be applied to trade accuracy for efficiency. &lt;/pre&gt; &lt;pre&gt;&lt;br /&gt;&lt;/pre&gt;&lt;pre&gt;In this paper, we make the first attempt of applying deep reinforcement learning to solve arithmetic word problems. The motivation is that deep Q-network has witnessed success in solving various problems with big search space and achieves promising performance in terms of both accuracy and running time. To fit the math problem scenario, we propose our MathDQN that is customized from the general deep reinforcement learning framework. Technically, we design the states, actions, reward function, together with a feed-forward neural network as the deep Q-network. Extensive experimental results validate our superiority over state-of-the-art methods. Our MathDQN yields remarkable improvement on most of datasets and boosts the average precision among all the benchmark datasets by 15\%.&lt;/pre&gt; &lt;/p&gt; },
    number={1},
    journal={Proceedings of the AAAI Conference on Artificial Intelligence},
    author={Wang, Lei and Zhang, Dongxiang and Gao, Lianli and Song, Jingkuan and Guo, Long and Shen, Heng Tao},
    year={2018},
    month={Apr.}
}

@misc{roy2016solvinggeneralarithmeticword,
      title={Solving General Arithmetic Word Problems}, 
      author={Subhro Roy and Dan Roth},
      year={2016},
      eprint={1608.01413},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1608.01413}, 
}

@article{koncel-kedziorski-etal-2015-parsing,
    title = "Parsing Algebraic Word Problems into Equations",
    author = "Koncel-Kedziorski, Rik  and
      Hajishirzi, Hannaneh  and
      Sabharwal, Ashish  and
      Etzioni, Oren  and
      Ang, Siena Dumas",
    editor = "Collins, Michael  and
      Lee, Lillian",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "3",
    year = "2015",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q15-1042/",
    doi = "10.1162/tacl_a_00160",
    pages = "585--597",
    abstract = "This paper formalizes the problem of solving multi-sentence algebraic word problems as that of generating and scoring equation trees. We use integer linear programming to generate equation trees and score their likelihood by learning local and global discriminative models. These models are trained on a small set of word problems and their answers, without any manual annotation, in order to choose the equation that best matches the problem text. We refer to the overall system as Alges. We compare Alges with previous work and show that it covers the full gamut of arithmetic operations whereas Hosseini et al. (2014) only handle addition and subtraction. In addition, Alges overcomes the brittleness of the Kushman et al. (2014) approach on single-equation problems, yielding a 15{\%} to 50{\%} reduction in error."
}

@inproceedings{mitra-baral-2016-learning,
    title = "Learning To Use Formulas To Solve Simple Arithmetic Problems",
    author = "Mitra, Arindam  and
      Baral, Chitta",
    editor = "Erk, Katrin  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1202/",
    doi = "10.18653/v1/P16-1202",
    pages = "2144--2153"
}

% % % Transformers and MoE -----------------------------------------------

@misc{bengio2016conditionalcomputationneuralnetworks,
      title={Conditional Computation in Neural Networks for faster models}, 
      author={Emmanuel Bengio and Pierre-Luc Bacon and Joelle Pineau and Doina Precup},
      year={2016},
      eprint={1511.06297},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1511.06297}, 
}

@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{jiang2024mixtralexperts,
      title={Mixtral of Experts}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Emma Bou Hanna and Florian Bressand and Gianna Lengyel and Guillaume Bour and Guillaume Lample and Lélio Renard Lavaud and Lucile Saulnier and Marie-Anne Lachaux and Pierre Stock and Sandeep Subramanian and Sophia Yang and Szymon Antoniak and Teven Le Scao and Théophile Gervet and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2024},
      eprint={2401.04088},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.04088}, 
}

@misc{jiang2023mistral7b,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825}, 
}

@misc{xue2024openmoeearlyeffortopen,
      title={OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models}, 
      author={Fuzhao Xue and Zian Zheng and Yao Fu and Jinjie Ni and Zangwei Zheng and Wangchunshu Zhou and Yang You},
      year={2024},
      eprint={2402.01739},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.01739}, 
}

@ARTICLE{6797059,
  author={Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
  journal={Neural Computation}, 
  title={Adaptive Mixtures of Local Experts}, 
  year={1991},
  volume={3},
  number={1},
  pages={79-87},
  keywords={},
  doi={10.1162/neco.1991.3.1.79},
}

@misc{lepikhin2020gshardscalinggiantmodels,
      title={GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding}, 
      author={Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and Maxim Krikun and Noam Shazeer and Zhifeng Chen},
      year={2020},
      eprint={2006.16668},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2006.16668}, 
}

@misc{fedus2022switchtransformersscalingtrillion,
      title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity}, 
      author={William Fedus and Barret Zoph and Noam Shazeer},
      year={2022},
      eprint={2101.03961},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2101.03961}, 
}

@misc{eigen2014learningfactoredrepresentationsdeep,
      title={Learning Factored Representations in a Deep Mixture of Experts}, 
      author={David Eigen and Marc'Aurelio Ranzato and Ilya Sutskever},
      year={2014},
      eprint={1312.4314},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1312.4314}, 
}

% % % CoT -----------------------------------------------

@misc{kojima2023largelanguagemodelszeroshot,
      title={Large Language Models are Zero-Shot Reasoners}, 
      author={Takeshi Kojima and Shixiang Shane Gu and Machel Reid and Yutaka Matsuo and Yusuke Iwasawa},
      year={2023},
      eprint={2205.11916},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2205.11916}, 
}
@misc{wei2023chainofthoughtpromptingelicitsreasoning,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2201.11903}, 
}

@misc{zhang2022automaticchainthoughtprompting,
      title={Automatic Chain of Thought Prompting in Large Language Models}, 
      author={Zhuosheng Zhang and Aston Zhang and Mu Li and Alex Smola},
      year={2022},
      eprint={2210.03493},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.03493}, 
}

@misc{shazeer2017outrageouslylargeneuralnetworks,
      title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}, 
      author={Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
      year={2017},
      eprint={1701.06538},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1701.06538}, 
}

@misc{devlin2019bertpretrainingdeepbidirectional,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}

@misc{yenduri2023generativepretrainedtransformercomprehensive,
      title={Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions}, 
      author={Gokul Yenduri and Ramalingam M and Chemmalar Selvi G and Supriya Y and Gautam Srivastava and Praveen Kumar Reddy Maddikunta and Deepti Raj G and Rutvij H Jhaveri and Prabadevi B and Weizheng Wang and Athanasios V. Vasilakos and Thippa Reddy Gadekallu},
      year={2023},
      eprint={2305.10435},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.10435}, 
}

% % % RL -----------------------------------------------

@misc{ouyang2022traininglanguagemodelsfollow,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.02155}, 
}

@misc{touvron2023llama2openfoundation,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Llama 2 Team},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.09288}, 
}

@misc{schulman2017proximalpolicyoptimizationalgorithms,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1707.06347}, 
}

@misc{ziegler2020finetuninglanguagemodelshuman,
      title={Fine-Tuning Language Models from Human Preferences}, 
      author={Daniel M. Ziegler and Nisan Stiennon and Jeffrey Wu and Tom B. Brown and Alec Radford and Dario Amodei and Paul Christiano and Geoffrey Irving},
      year={2020},
      eprint={1909.08593},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.08593}, 
}

@misc{rafailov2024directpreferenceoptimizationlanguage,
      title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model}, 
      author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
      year={2024},
      eprint={2305.18290},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.18290}, 
}

@misc{hejna2024contrastivepreferencelearninglearning,
      title={Contrastive Preference Learning: Learning from Human Feedback without RL}, 
      author={Joey Hejna and Rafael Rafailov and Harshit Sikchi and Chelsea Finn and Scott Niekum and W. Bradley Knox and Dorsa Sadigh},
      year={2024},
      eprint={2310.13639},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.13639}, 
}

@misc{gulcehre2023reinforcedselftrainingrestlanguage,
      title={Reinforced Self-Training (ReST) for Language Modeling}, 
      author={Caglar Gulcehre and Tom Le Paine and Srivatsan Srinivasan and Ksenia Konyushkova and Lotte Weerts and Abhishek Sharma and Aditya Siddhant and Alex Ahern and Miaosen Wang and Chenjie Gu and Wolfgang Macherey and Arnaud Doucet and Orhan Firat and Nando de Freitas},
      year={2023},
      eprint={2308.08998},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.08998}, 
}

@misc{bai2022constitutionalaiharmlessnessai,
      title={Constitutional AI: Harmlessness from AI Feedback}, 
      author={Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and Jackson Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Cameron McKinnon and Carol Chen and Catherine Olsson and Christopher Olah and Danny Hernandez and Dawn Drain and Deep Ganguli and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jamie Kerr and Jared Mueller and Jeffrey Ladish and Joshua Landau and Kamal Ndousse and Kamile Lukosuite and Liane Lovitt and Michael Sellitto and Nelson Elhage and Nicholas Schiefer and Noemi Mercado and Nova DasSarma and Robert Lasenby and Robin Larson and Sam Ringer and Scott Johnston and Shauna Kravec and Sheer El Showk and Stanislav Fort and Tamera Lanham and Timothy Telleen-Lawton and Tom Conerly and Tom Henighan and Tristan Hume and Samuel R. Bowman and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Jared Kaplan},
      year={2022},
      eprint={2212.08073},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.08073}, 
}

@misc{zhang2023wisdomhindsightmakeslanguage,
      title={The Wisdom of Hindsight Makes Language Models Better Instruction Followers}, 
      author={Tianjun Zhang and Fangchen Liu and Justin Wong and Pieter Abbeel and Joseph E. Gonzalez},
      year={2023},
      eprint={2302.05206},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.05206}, 
}

@misc{lee2024rlaifvsrlhfscaling,
      title={RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback}, 
      author={Harrison Lee and Samrat Phatale and Hassan Mansoor and Thomas Mesnard and Johan Ferret and Kellie Lu and Colton Bishop and Ethan Hall and Victor Carbune and Abhinav Rastogi and Sushant Prakash},
      year={2024},
      eprint={2309.00267},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.00267}, 
}

% % % RAG -----------------------------------------------

@misc{levonian2023retrievalaugmentedgenerationimprovemath,
      title={Retrieval-augmented Generation to Improve Math Question-Answering: Trade-offs Between Groundedness and Human Preference}, 
      author={Zachary Levonian and Chenglu Li and Wangda Zhu and Anoushka Gade and Owen Henkel and Millie-Ellen Postle and Wanli Xing},
      year={2023},
      eprint={2310.03184},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.03184}, 
}

% % % Reasoning models ----------------------------------------------

@misc{zelikman2022starbootstrappingreasoningreasoning,
      title={STaR: Bootstrapping Reasoning With Reasoning}, 
      author={Eric Zelikman and Yuhuai Wu and Jesse Mu and Noah D. Goodman},
      year={2022},
      eprint={2203.14465},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2203.14465}, 
}

@misc{zelikman2024quietstarlanguagemodelsteach,
      title={Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking}, 
      author={Eric Zelikman and Georges Harik and Yijia Shao and Varuna Jayasiri and Nick Haber and Noah D. Goodman},
      year={2024},
      eprint={2403.09629},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.09629}, 
}

@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI Team},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}

@misc{kimiteam2025kimik15scalingreinforcement,
      title={Kimi k1.5: Scaling Reinforcement Learning with LLMs}, 
      author={Kimi Team},
      year={2025},
      eprint={2501.12599},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2501.12599}, 
}


% % % ToRA -----------------------------------------------

@misc{bubeck2023sparksartificialgeneralintelligence,
      title={Sparks of Artificial General Intelligence: Early experiments with GPT-4}, 
      author={Sébastien Bubeck and Varun Chandrasekaran and Ronen Eldan and Johannes Gehrke and Eric Horvitz and Ece Kamar and Peter Lee and Yin Tat Lee and Yuanzhi Li and Scott Lundberg and Harsha Nori and Hamid Palangi and Marco Tulio Ribeiro and Yi Zhang},
      year={2023},
      eprint={2303.12712},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.12712}, 
}

@misc{gao2023palprogramaidedlanguagemodels,
  title = {PAL: Program-aided language models},
  author = {Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  year = {2023},
  eprint = {2211.10435},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url = {https://arxiv.org/abs/2211.10435},
}

@misc{gou2024toratoolintegratedreasoningagent,
    title={ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving}, 
    author={Zhibin Gou and Zhihong Shao and Yeyun Gong and Yelong Shen and Yujiu Yang and Minlie Huang and Nan Duan and Weizhu Chen},
    year={2024},
    eprint={2309.17452},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2309.17452} 
}

@article{Trinh2024SolvingOG,
  title={Solving olympiad geometry without human demonstrations},
  author={Trieu H. Trinh and Yuhuai Wu and Quoc V. Le and He He and Thang Luong},
  journal={Nature},
  year={2024},
  volume={625},
  pages={476 - 482},
  url={https://api.semanticscholar.org/CorpusID:267032902}
}

% % % 2 DATASETS -----------------------------------------------

@misc{cobbe2021trainingverifierssolvemath,
    title={Training Verifiers to Solve Math Word Problems}, 
    author={Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
    year={2021},
    eprint={2110.14168},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2110.14168}, 
}

@misc{hendrycks2021measuringmathematicalproblemsolving,
      title={Measuring Mathematical Problem Solving With the MATH Dataset}, 
      author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
      year={2021},
      eprint={2103.03874},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2103.03874}, 
}

@misc{hendrycks2021measuringmassivemultitasklanguage,
      title={Measuring Massive Multitask Language Understanding}, 
      author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
      year={2021},
      eprint={2009.03300},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2009.03300}, 
}

@misc{lu2024mathgeniegeneratingsyntheticdata,
      title={MathGenie: Generating Synthetic Data with Question Back-translation for Enhancing Mathematical Reasoning of LLMs}, 
      author={Zimu Lu and Aojun Zhou and Houxing Ren and Ke Wang and Weikang Shi and Junting Pan and Mingjie Zhan and Hongsheng Li},
      year={2024},
      eprint={2402.16352},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.16352}, 
}

@misc{numina_math_7b,
  author = {Edward Beeching and Shengyi Costa Huang and Albert Jiang and Jia Li and Benjamin Lipkin and Zihan Qina and Kashif Rasul and Ziju Shen and Roman Soletskyi and Lewis Tunstall},
  title = {NuminaMath 7B TIR},
  year = {2024},
  publisher = {Numina & Hugging Face},
  journal = {Hugging Face repository},
  howpublished = {\url{https://huggingface.co/AI-MO/NuminaMath-7B-TIR}}
}

@inproceedings{kushman-etal-2014-learning,
    title = "Learning to Automatically Solve Algebra Word Problems",
    author = "Kushman, Nate  and
      Artzi, Yoav  and
      Zettlemoyer, Luke  and
      Barzilay, Regina",
    editor = "Toutanova, Kristina  and
      Wu, Hua",
    booktitle = "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P14-1026/",
    doi = "10.3115/v1/P14-1026",
    pages = "271--281"
}

@inproceedings{hosseini-etal-2014-learning,
    title = "Learning to Solve Arithmetic Word Problems with Verb Categorization",
    author = "Hosseini, Mohammad Javad  and
      Hajishirzi, Hannaneh  and
      Etzioni, Oren  and
      Kushman, Nate",
    editor = "Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1058/",
    doi = "10.3115/v1/D14-1058",
    pages = "523--533"
}

@inproceedings{shi-etal-2015-automatically,
    title = "Automatically Solving Number Word Problems by Semantic Parsing and Reasoning",
    author = "Shi, Shuming  and
      Wang, Yuehui  and
      Lin, Chin-Yew  and
      Liu, Xiaojiang  and
      Rui, Yong",
    editor = "M{\`a}rquez, Llu{\'i}s  and
      Callison-Burch, Chris  and
      Su, Jian",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1135/",
    doi = "10.18653/v1/D15-1135",
    pages = "1132--1142"
}

@misc{Upadhyay2015DRAWAC,
  title={DRAW: A Challenging and Diverse Algebra Word Problem Set},
  author={Shyam Upadhyay and Ming-Wei Chang},
  year={2015},
  url={https://api.semanticscholar.org/CorpusID:56036017}
}

@article{koncel2015parsing,
  title={Parsing algebraic word problems into equations},
  author={Koncel-Kedziorski, Rik and Hajishirzi, Hannaneh and Sabharwal, Ashish and Etzioni, Oren and Ang, Siena Dumas},
  journal={Transactions of the Association for Computational Linguistics},
  volume={3},
  pages={585--597},
  year={2015},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{koncel-kedziorski-etal-2016-mawps,
    title = "{MAWPS}: A Math Word Problem Repository",
    author = "Koncel-Kedziorski, Rik  and
      Roy, Subhro  and
      Amini, Aida  and
      Kushman, Nate  and
      Hajishirzi, Hannaneh",
    editor = "Knight, Kevin  and
      Nenkova, Ani  and
      Rambow, Owen",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1136/",
    doi = "10.18653/v1/N16-1136",
    pages = "1152--1157"
}

@inproceedings{huang-etal-2016-well,
    title = "How well do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation",
    author = "Huang, Danqing  and
      Shi, Shuming  and
      Lin, Chin-Yew  and
      Yin, Jian  and
      Ma, Wei-Ying",
    editor = "Erk, Katrin  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1084/",
    doi = "10.18653/v1/P16-1084",
    pages = "887--896"
}

@inproceedings{wang-etal-2017-deep,
    title = "Deep Neural Solver for Math Word Problems",
    author = "Wang, Yan  and
      Liu, Xiaojiang  and
      Shi, Shuming",
    editor = "Palmer, Martha  and
      Hwa, Rebecca  and
      Riedel, Sebastian",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1088/",
    doi = "10.18653/v1/D17-1088",
    pages = "845--854",
    abstract = "This paper presents a deep neural solver to automatically solve math word problems. In contrast to previous statistical learning approaches, we directly translate math word problems to equation templates using a recurrent neural network (RNN) model, without sophisticated feature engineering. We further design a hybrid model that combines the RNN model and a similarity-based retrieval model to achieve additional performance improvement. Experiments conducted on a large dataset show that the RNN model and the hybrid model significantly outperform state-of-the-art statistical learning methods for math word problem solving."
}

@misc{zhao2020ape210klargescaletemplaterichdataset,
      title={Ape210K: A Large-Scale and Template-Rich Dataset of Math Word Problems}, 
      author={Wei Zhao and Mingyue Shang and Yang Liu and Liang Wang and Jingming Liu},
      year={2020},
      eprint={2009.11506},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2009.11506}, 
}

@inproceedings{patel-etal-2021-nlp,
    title = "Are {NLP} Models really able to Solve Simple Math Word Problems?",
    author = "Patel, Arkil  and
      Bhattamishra, Satwik  and
      Goyal, Navin",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.168/",
    doi = "10.18653/v1/2021.naacl-main.168",
    pages = "2080--2094",
    abstract = "The problem of designing NLP solvers for math word problems (MWP) has seen sustained research activity and steady gains in the test accuracy. Since existing solvers achieve high performance on the benchmark datasets for elementary level MWPs containing one-unknown arithmetic word problems, such problems are often considered {\textquotedblleft}solved{\textquotedblright} with the bulk of research attention moving to more complex MWPs. In this paper, we restrict our attention to English MWPs taught in grades four and lower. We provide strong evidence that the existing MWP solvers rely on shallow heuristics to achieve high performance on the benchmark datasets. To this end, we show that MWP solvers that do not have access to the question asked in the MWP can still solve a large fraction of MWPs. Similarly, models that treat MWPs as bag-of-words can also achieve surprisingly high accuracy. Further, we introduce a challenge dataset, SVAMP, created by applying carefully chosen variations over examples sampled from existing datasets. The best accuracy achieved by state-of-the-art models is substantially lower on SVAMP, thus showing that much remains to be done even for the simplest of the MWPs."
}

@misc{liu2023fimochallengeformaldataset,
      title={FIMO: A Challenge Formal Dataset for Automated Theorem Proving}, 
      author={Chengwu Liu and Jianhao Shen and Huajian Xin and Zhengying Liu and Ye Yuan and Haiming Wang and Wei Ju and Chuanyang Zheng and Yichun Yin and Lin Li and Ming Zhang and Qun Liu},
      year={2023},
      eprint={2309.04295},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2309.04295}, 
}

@misc{zheng2022minif2fcrosssystembenchmarkformal,
      title={MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics}, 
      author={Kunhao Zheng and Jesse Michael Han and Stanislas Polu},
      year={2022},
      eprint={2109.00110},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2109.00110}, 
}

@misc{mittal2024puzzlebenchllmssolvechallenging,
    title={PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?}, 
    author={Chinmay Mittal and Krishna Kartik and Mausam and Parag Singla},
    year={2024},
    eprint={2402.02611},
    archivePrefix={arXiv},
    primaryClass={cs.AI},
    url={https://arxiv.org/abs/2402.02611}, 
}

@misc{patel2021nlpmodelsreallyable,
      title={Are NLP Models really able to Solve Simple Math Word Problems?}, 
      author={Arkil Patel and Satwik Bhattamishra and Navin Goyal},
      year={2021},
      eprint={2103.07191},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2103.07191},
}

@misc{frieder2023mathematicalcapabilitieschatgpt,
    title={Mathematical Capabilities of ChatGPT}, 
    author={Simon Frieder and Luca Pinchetti and Alexis Chevalier and Ryan-Rhys Griffiths and Tommaso Salvatori and Thomas Lukasiewicz and Philipp Christian Petersen and Julius Berner},
    year={2023},
    eprint={2301.13867},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2301.13867}, 
}

@misc{collins2023evaluatinglanguagemodelsmathematics,
      title={Evaluating Language Models for Mathematics through Interactions}, 
      author={Katherine M. Collins and Albert Q. Jiang and Simon Frieder and Lionel Wong and Miri Zilka and Umang Bhatt and Thomas Lukasiewicz and Yuhuai Wu and Joshua B. Tenenbaum and William Hart and Timothy Gowers and Wenda Li and Adrian Weller and Mateja Jamnik},
      year={2023},
      eprint={2306.01694},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.01694}, 
}

@misc{zhang2024carefulexaminationlargelanguage,
    title={A Careful Examination of Large Language Model Performance on Grade School Arithmetic}, 
    author={Hugh Zhang and Jeff Da and Dean Lee and Vaughn Robinson and Catherine Wu and Will Song and Tiffany Zhao and Pranav Raja and Dylan Slack and Qin Lyu and Sean Hendryx and Russell Kaplan and Michele Lunati and Summer Yue},
    year={2024},
    eprint={2405.00332},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2405.00332}, 
}

@misc{wang2024mmluprorobustchallengingmultitask,
    title={MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark}, 
    author={Yubo Wang and Xueguang Ma and Ge Zhang and Yuansheng Ni and Abhranil Chandra and Shiguang Guo and Weiming Ren and Aaran Arulraj and Xuan He and Ziyan Jiang and Tianle Li and Max Ku and Kai Wang and Alex Zhuang and Rongqi Fan and Xiang Yue and Wenhu Chen},
    year={2024},
    eprint={2406.01574},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2406.01574}, 
}

@misc{zheng2023judgingllmasajudgemtbenchchatbot,
      title={Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena}, 
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric P. Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.05685}, 
}

@misc{mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical,
      title={GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models}, 
      author={Iman Mirzadeh and Keivan Alizadeh and Hooman Shahrokhi and Oncel Tuzel and Samy Bengio and Mehrdad Farajtabar},
      year={2024},
      eprint={2410.05229},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.05229}, 
}

@misc{golchin2024timetravelllmstracing,
      title={Time Travel in LLMs: Tracing Data Contamination in Large Language Models}, 
      author={Shahriar Golchin and Mihai Surdeanu},
      year={2024},
      eprint={2308.08493},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.08493}, 
}

@misc{ye2024physicslanguagemodels21,
      title={Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process}, 
      author={Tian Ye and Zicheng Xu and Yuanzhi Li and Zeyuan Allen-Zhu},
      year={2024},
      eprint={2407.20311},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.20311}, 
}

@misc{he2024olympiadbenchchallengingbenchmarkpromoting,
      title={OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems}, 
      author={Chaoqun He and Renjie Luo and Yuzhuo Bai and Shengding Hu and Zhen Leng Thai and Junhao Shen and Jinyi Hu and Xu Han and Yujie Huang and Yuxiang Zhang and Jie Liu and Lei Qi and Zhiyuan Liu and Maosong Sun},
      year={2024},
      eprint={2402.14008},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.14008}, 
}

@misc{gao2024omnimathuniversalolympiadlevel,
      title={Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models}, 
      author={Bofei Gao and Feifan Song and Zhe Yang and Zefan Cai and Yibo Miao and Qingxiu Dong and Lei Li and Chenghao Ma and Liang Chen and Runxin Xu and Zhengyang Tang and Benyou Wang and Daoguang Zan and Shanghaoran Quan and Ge Zhang and Lei Sha and Yichang Zhang and Xuancheng Ren and Tianyu Liu and Baobao Chang},
      year={2024},
      eprint={2410.07985},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.07985}, 
}

@misc{mao2024champcompetitionleveldatasetfinegrained,
      title={CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities}, 
      author={Yujun Mao and Yoon Kim and Yilun Zhou},
      year={2024},
      eprint={2401.06961},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.06961}, 
}

% % % 3 EXPERIMENTS -----------------------------------------------

@article{Anisimov2005,
    author={Anisimov, A. V. and Derevyanchenko, A. V.},
    title={The System PARCS-JAVA for Parallel Computations on Computer Networks},
    journal={Cybernetics and Systems Analysis},
    year={2005},
    month={Jan},
    day={01},
    volume={41},
    number={1},
    pages={17-26},
    abstract={The system PARCS-JAVA provides software tools for solution of problems on computer networks. It can be installed on heterogeneous computer networks and allows users of small computers to use parallel data processing.},
    issn={1573-8337},
    doi={10.1007/s10559-005-0037-4},
    url={https://doi.org/10.1007/s10559-005-0037-4}
}

@misc{jacob2017quantizationtrainingneuralnetworks,
      title={Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference}, 
      author={Benoit Jacob and Skirmantas Kligys and Bo Chen and Menglong Zhu and Matthew Tang and Andrew Howard and Hartwig Adam and Dmitry Kalenichenko},
      year={2017},
      eprint={1712.05877},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1712.05877}, 
}

@misc{grattafiori2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Llama Team},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}
